import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# --- 1. é…ç½®å’Œæ¨¡å‹åŠ è½½ ---

st.set_page_config(
    page_title="LearnLM-nano Chat",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.sidebar.title("æ¨¡å‹é…ç½®")
base_model_path = st.sidebar.text_input(
    "åŸºç¡€æ¨¡å‹è·¯å¾„ (Base Model Path)",
    value="/root/autodl-tmp/qwen/Qwen2.5-7B-Instruct"
)
lora_checkpoint_path = st.sidebar.text_input(
    "LoRA æƒé‡è·¯å¾„ (LoRA Checkpoint Path)",
    value="./output/LearnLM-nano/checkpoint-600" # è„šæœ¬ä¸­æµ‹è¯•ç”¨çš„checkpoint
)

SYSTEM_PROMPT = "ä½ æ˜¯ LearnLMï¼Œä¸€ä¸ªæœ‰ç”¨çš„äººå·¥æ™ºèƒ½æ•™å­¦åŠ©æ‰‹ã€‚"

@st.cache_resource
def load_model(base_path, lora_path):
    """
    åŠ è½½åŸºç¡€æ¨¡å‹å’Œ LoRA æƒé‡ï¼Œå¹¶è¿”å›åˆå¹¶åçš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    """
    try:
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            base_path,
            trust_remote_code=True,
            use_fast=False
        )
        # Qwen2 tokenizer çš„ pad_token é»˜è®¤ä¸º <|endoftext|>
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        ).eval()

        # åŠ è½½å¹¶èåˆ LoRA æƒé‡
        # PeftModel ä¼šè‡ªåŠ¨å¤„ç†æƒé‡åˆå¹¶ï¼Œä»¥ä¾¿è¿›è¡Œé«˜æ•ˆæ¨ç†
        model = PeftModel.from_pretrained(base_model, model_id=lora_path)
        
        return model, tokenizer
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®: {e}")
        return None, None

# --- 2. UI ç•Œé¢å’ŒèŠå¤©é€»è¾‘ ---

st.title("ğŸ¤– LearnLM-nano ChatBot")
# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
with st.spinner("æ­£åœ¨åŠ è½½ LearnLM-nano æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
    model, tokenizer = load_model(base_model_path, lora_checkpoint_path)

if model and tokenizer:
    # åˆå§‹åŒ–èŠå¤©è®°å½•
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
    # åœ¨ä¾§è¾¹æ æ·»åŠ ä¸€ä¸ªæ¸…é™¤å¯¹è¯çš„æŒ‰é’®
    if st.sidebar.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

    # æ¥æ”¶ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("ä½ å¥½ï¼Œæˆ‘æ˜¯ LearnLMï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°èŠå¤©è®°å½•å¹¶æ˜¾ç¤º
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        # 1. åˆ›å»ºåŒ…å«ç³»ç»Ÿæç¤ºå’Œæ‰€æœ‰å¯¹è¯å†å²çš„åˆ—è¡¨
        #    æ³¨æ„ï¼šæ¯æ¬¡éƒ½å°†å®Œæ•´çš„å†å²è®°å½•ä¼ é€’ç»™æ¨¡å‹ï¼Œä»¥ä¾¿å®ƒç†è§£ä¸Šä¸‹æ–‡
        chat_history = [
            {"role": "system", "content": SYSTEM_PROMPT}
        ] + st.session_state.messages
        
        # 2. ä½¿ç”¨ apply_chat_template æ ¼å¼åŒ–è¾“å…¥
        inputs = tokenizer.apply_chat_template(
            chat_history,
            add_generation_prompt=True, # è‡ªåŠ¨æ·»åŠ  assistant è§’è‰²çš„èµ·å§‹ç¬¦
            tokenize=True,
            return_tensors="pt",
        ).to(model.device)

        # 3. ç”Ÿæˆå›å¤
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                # è®¾ç½®ç”Ÿæˆå‚æ•°
                gen_kwargs = {"max_new_tokens": 512, "do_sample": True, "top_k": 5, "top_p": 0.9}
                
                with torch.no_grad():
                    outputs = model.generate(input_ids=inputs, **gen_kwargs)
                    # ä»æ¨¡å‹è¾“å‡ºä¸­åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                    response_ids = outputs[0, inputs.shape[1]:]
                    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)

            # æ˜¾ç¤ºæ¨¡å‹çš„å›å¤
            st.markdown(response_text)
        
        # å°†æ¨¡å‹çš„å›å¤ä¹Ÿæ·»åŠ åˆ°èŠå¤©è®°å½•
        st.session_state.messages.append({"role": "assistant", "content": response_text})
else:
    st.warning("æ¨¡å‹æœªèƒ½æˆåŠŸåŠ è½½ï¼Œè¯·æ£€æŸ¥ä¾§è¾¹æ ä¸­çš„è·¯å¾„é…ç½®ã€‚")
